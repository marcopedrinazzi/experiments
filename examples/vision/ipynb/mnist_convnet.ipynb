{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcopedrinazzi/experiments/blob/main/examples/vision/ipynb/mnist_convnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGOggN1U43MG"
      },
      "source": [
        "# Simple MNIST convnet\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2015/06/19<br>\n",
        "**Last modified:** 2020/04/21<br>\n",
        "**Description:** A simple convnet that achieves ~99% test accuracy on MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKOHsGNE43MN"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uQdniBSh43MO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJJF-EW343MR"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TUAE_zJb43MR",
        "outputId": "8316bd74-4ed4-41e1-dd24-d90963f85a57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "x_test shape: (10000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10 #da 0 a 9\n",
        "input_shape = (28, 28, 1) #(Width, Height, Channels) 1 per grigio 3 per RGB\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "#So the x_train data is the \"features.\" In this case, the features are pixel \n",
        "#values of the 28x28 images of these digits 0-9. \n",
        "#The y_train is the label (is it a 0,1,2,3,4,5,6,7,8 or a 9?)\n",
        "#The testing variants of these variables is the \"out of sample\" examples that \n",
        "#we will use. These are examples from our data that we're going to set aside, \n",
        "#reserving them for testing the model.\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbREWZ_o43MT"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c18e9pgF43MU",
        "outputId": "f73c80d6-dded-426b-99b8-1847bdcc3faf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                16010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"), #num_classes è il\n",
        "        #\n",
        "    ]\n",
        ")\n",
        "\n",
        "#A sequential model is what you're going to use most of the time. It just means \n",
        "#things are going to go in direct order. A feed forward model. \n",
        "\n",
        "#layers.Conv2D(32,....)\n",
        "#Fa la convoluzione 32 è il numero di filtri\n",
        "#L'output sarà dopo la prima convoluzione di 26x26 (muovo il kernel 3x3 per \n",
        "#tutta l'immagine)\n",
        "\n",
        "#layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "#Fa il Max pooling e l'ouput della immagine sarà 13x13 perchè setto nella \n",
        "#immagine da 26x26 in pool da 2x2\n",
        "\n",
        "#For this you need to understand what filters actually do.\n",
        "\n",
        "#Nella nuova convoluzione aumento i filtri perchè:\n",
        "#Every layer of filters is there to capture patterns. For example, the first \n",
        "#layer of filters captures patterns like edges, corners, dots etc. Subsequent \n",
        "#layers combine those patterns to make bigger patterns \n",
        "#(like combining edges to make squares, circles, etc.).\n",
        "#Now as we move forward in the layers, the patterns get more complex; \n",
        "#hence there are larger combinations of patterns to capture. \n",
        "#That's why we increase the filter size in subsequent layers to capture as many \n",
        "#combinations as possible.\n",
        "\n",
        "# layers.Flatten(),\n",
        "# Fa il flattening\n",
        "\n",
        "# layers.Dropout(0.5),\n",
        "#Applies Dropout to the input.\n",
        "#The Dropout layer randomly sets input units to 0 with a frequency of rate \n",
        "#at each step during training time, which helps prevent overfitting. \n",
        "#Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all \n",
        "#inputs is unchanged.\n",
        "\n",
        "#layers.Dense(num_classes, activation=\"softmax\"),\n",
        "#E' il layer fully connected che serve per la classificazione finale\n",
        "\n",
        "#https://towardsdatascience.com/convolution-neural-network-for-image-processing-using-keras-dc3429056306\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxkN80Tr43MV"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uUmXCTTC43MV",
        "outputId": "5303df4f-b2ca-4002-c8bc-6effdf397a9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "422/422 [==============================] - 46s 108ms/step - loss: 0.3732 - accuracy: 0.8887 - val_loss: 0.0830 - val_accuracy: 0.9792\n",
            "Epoch 2/15\n",
            "422/422 [==============================] - 41s 98ms/step - loss: 0.1167 - accuracy: 0.9646 - val_loss: 0.0608 - val_accuracy: 0.9850\n",
            "Epoch 3/15\n",
            "422/422 [==============================] - 41s 97ms/step - loss: 0.0891 - accuracy: 0.9723 - val_loss: 0.0507 - val_accuracy: 0.9883\n",
            "Epoch 4/15\n",
            "422/422 [==============================] - 41s 97ms/step - loss: 0.0736 - accuracy: 0.9774 - val_loss: 0.0418 - val_accuracy: 0.9885\n",
            "Epoch 5/15\n",
            "422/422 [==============================] - 41s 97ms/step - loss: 0.0649 - accuracy: 0.9805 - val_loss: 0.0404 - val_accuracy: 0.9887\n",
            "Epoch 6/15\n",
            "422/422 [==============================] - 40s 96ms/step - loss: 0.0589 - accuracy: 0.9815 - val_loss: 0.0408 - val_accuracy: 0.9893\n",
            "Epoch 7/15\n",
            "422/422 [==============================] - 40s 96ms/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0374 - val_accuracy: 0.9902\n",
            "Epoch 8/15\n",
            "422/422 [==============================] - 41s 96ms/step - loss: 0.0491 - accuracy: 0.9848 - val_loss: 0.0344 - val_accuracy: 0.9902\n",
            "Epoch 9/15\n",
            "422/422 [==============================] - 41s 97ms/step - loss: 0.0461 - accuracy: 0.9857 - val_loss: 0.0357 - val_accuracy: 0.9903\n",
            "Epoch 10/15\n",
            "422/422 [==============================] - 42s 100ms/step - loss: 0.0438 - accuracy: 0.9862 - val_loss: 0.0310 - val_accuracy: 0.9927\n",
            "Epoch 11/15\n",
            "422/422 [==============================] - 42s 99ms/step - loss: 0.0422 - accuracy: 0.9865 - val_loss: 0.0337 - val_accuracy: 0.9910\n",
            "Epoch 12/15\n",
            "422/422 [==============================] - 41s 98ms/step - loss: 0.0394 - accuracy: 0.9875 - val_loss: 0.0312 - val_accuracy: 0.9920\n",
            "Epoch 13/15\n",
            "422/422 [==============================] - 41s 98ms/step - loss: 0.0365 - accuracy: 0.9880 - val_loss: 0.0300 - val_accuracy: 0.9920\n",
            "Epoch 14/15\n",
            "422/422 [==============================] - 45s 106ms/step - loss: 0.0354 - accuracy: 0.9883 - val_loss: 0.0313 - val_accuracy: 0.9912\n",
            "Epoch 15/15\n",
            "422/422 [==============================] - 42s 99ms/step - loss: 0.0340 - accuracy: 0.9891 - val_loss: 0.0283 - val_accuracy: 0.9923\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb46b0aed0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#FACCIO IL TRAINING FACCIO UNA SEPARAZIONE ULTERIORE NEI TRAINING DATA UTILIZZANDO PARTE DEI TRAINING DATA COME VALIDATION DATA (PARAMETRO VALIDATION_SPLIT)\n",
        "#validation_split says: \"hey give me all the input data – I will take care of splitting between test and validation\".\n",
        "#validation_data says \"please give me explicitly the validation data\"\n",
        "#https://stackoverflow.com/questions/45117295/what-is-the-relation-between-validation-data-and-validation-split-in-keras-fit\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "#The batch size defines the number of samples that will be propagated through the network.\n",
        "#https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
        "#QUI LO SPIEGA BENE\n",
        "epochs = 15\n",
        "\n",
        "#Now we need to \"compile\" the model. This is where we pass the settings for \n",
        "#actually optimizing/training the model we've defined.\n",
        "\n",
        "#Next, we have our loss metric. Loss is a calculation of error. \n",
        "#A neural network doesn't actually attempt to maximize accuracy. \n",
        "#It attempts to minimize loss. Again, there are many choices, but some form of \n",
        "#categorical crossentropy is a good start for a classification task like this.\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "#As we train, we can see loss goes down (yay), and accuracy improves quite quickly to 98-99% (double yay!)\n",
        "\n",
        "#batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32. \n",
        "#Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches).\n",
        "\n",
        "#validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. \n",
        "#The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. \n",
        "#The validation data is selected from the last samples in the x and y data provided, before shuffling. \n",
        "#This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5GvrHhI43MW"
      },
      "source": [
        "## Evaluate the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HWigkB1343MW",
        "outputId": "c6c97602-92c2-4120-c4e4-27e4e25b640e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.024355581030249596\n",
            "Test accuracy: 0.991599977016449\n"
          ]
        }
      ],
      "source": [
        "#FACCIO TEST SUI VALORI DEL DATASET CHE NON SONO MAI STATI USATI IN ALLENAMENTO\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "\n",
        "#Now that's loss and accuracy for in-sample data. Getting a high accuracy and \n",
        "#low loss might mean your model learned how to classify digits in general (it generalized)...or it simply memorized every single example you showed it (it overfit). \n",
        "#This is why we need to test on out-of-sample data (data we didn't use to train the model)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model and load it back (in the next section)\n"
      ],
      "metadata": {
        "id": "np_ScLhFAf6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('cnn1.model')"
      ],
      "metadata": {
        "id": "0Ro7jAAzAe6f",
        "outputId": "4d02ac55-ac2c-45fa-83ce-9dbf9afc0b45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: cnn1.model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions\n"
      ],
      "metadata": {
        "id": "vR8W2-ivARL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = keras.models.load_model('cnn1.model')\n",
        "predictions = new_model.predict(x_test)\n",
        "\n",
        "print(\"Ho previsto questo numero\",np.argmax(predictions[1]))\n",
        "\n",
        "print(\"Il numero inserito era questo\")\n",
        "\n",
        "input_di_test = np.squeeze(x_test[1]) #Remove axes of length one from a. Cosi posso fare il grafico\n",
        "plt.imshow(input_di_test,cmap=plt.cm.binary)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-O6vbIX2_4SU",
        "outputId": "5a056b4b-90b4-4f56-b032-054c9a75ba04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ho previsto questo numero 2\n",
            "Il numero inserito era questo\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mnist_convnet",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}